{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This notebook is for experimenting with the open graph benchmarks(ogb) node prediction tasks.\n",
    "* It contains the following modules\n",
    "    * analysis-does simple eda\n",
    "    * create_dataloader- creates a train valid and test dataloader\n",
    "    * Gat- GatConv layers.\n",
    "    * one_epoch- trains model for one epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from ogb.nodeproppred import PygNodePropPredDataset, Evaluator\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.data.sampler import NeighborSampler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataloader/analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a folder called node_dataset and all datasets will be downloaded there, if they don't exist.\n",
    "dataset=PygNodePropPredDataset(name='ogbn-arxiv',root='../node_dataset/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_idx = dataset.get_idx_split()#a dictonary with train,valid and test ids\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis(data,split_idx:dict,name:str):\n",
    "    if name=='ogbn-arxiv' or name=='ogbn-products':\n",
    "        print(f'output analysis {name}')\n",
    "        for i in ['train','valid','test']:\n",
    "            figure=plt.figure(figsize=(10,3))\n",
    "            sns.countplot(x=data.y[split_idx[i]].squeeze(1).numpy())\n",
    "            plt.title(f'{i} y')\n",
    "            plt.show()\n",
    "    return data\n",
    "\n",
    "def preprocess(data,split_idx:dict,name:str):\n",
    "    '''\n",
    "    Normalize node features.\n",
    "    preprocess dataset based on name.\n",
    "    --args--\n",
    "    data=PygNodePropPredDataset(name)[0]\n",
    "    name: name of ogbn dataset.\n",
    "    '''\n",
    "    print(f'preprocessing {name}')\n",
    "    if name=='ogbn-arxiv':\n",
    "        #add directed edge in other direction\n",
    "        #add edge attribute\n",
    "        print('adding edges in other direction')\n",
    "        print('normalizing')\n",
    "    elif name=='ogbn-products':\n",
    "        print('normalizing')\n",
    "    return data\n",
    "\n",
    "def create_dataloder(data,split_idx:dict,sizes=[-1,-1,-1],batch_size=2048)->dict:\n",
    "    '''\n",
    "    return train,test and valid dataloaders in a dict.\n",
    "    --args--\n",
    "    data=PygNodePropPredDataset(name)[0]\n",
    "    split_idx: is a dictonary with train,valid and test ids. output of get_idx_split\n",
    "    sizes: len(sizes)=number of GNN layers. -1 implies no downsampling at that layer.\n",
    "    '''\n",
    "    loader_dict={}\n",
    "    train_batchsize=batch_size\n",
    "    for i in ['train','valid','test']:\n",
    "        idx=split_idx[i]\n",
    "        batch_size=train_batchsize if i=='train' else 2*train_batchsize\n",
    "        shuffle=True if i=='train' else False\n",
    "        loader=NeighborSampler(data.edge_index,node_idx=idx,sizes=sizes, batch_size=batch_size,\n",
    "                               shuffle=shuffle)\n",
    "        loader_dict[i]=loader\n",
    "    return loader_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GNN models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_geometric\n",
    "from torch_geometric.nn import GATConv,BatchNorm\n",
    "from torch import nn,optim\n",
    "class Gat(nn.Module):\n",
    "    def __init__(self,inp_dim=3,filters=[16,16,16],heads=[2,2,2],drop=0.1,edge_drop=0.1,bn=True,skip=True):\n",
    "        super().__init__()#all params are added to _modules internally, this makes sure its initialized.\n",
    "        assert len(heads)==len(filters)\n",
    "        self.gat_modules=nn.ModuleList()\n",
    "        self.bn=bn\n",
    "        self.skip=skip\n",
    "        if self.bn:\n",
    "            self.bn_modules=nn.ModuleList()\n",
    "        if self.skip:\n",
    "            self.skip_conns=nn.ModuleList()\n",
    "\n",
    "        for i,j in enumerate(filters):\n",
    "            if i==0:\n",
    "                self.gat_modules.append(GATConv(in_channels=inp_dim,out_channels=filters[i],heads=heads[i],concat=False,dropout=edge_drop))\n",
    "            else:\n",
    "                self.gat_modules.append(GATConv(in_channels=filters[i-1],out_channels=filters[i],heads=heads[i],concat=False,dropout=edge_drop))\n",
    "            \n",
    "            if self.bn:\n",
    "                bn_dim=filters[i]\n",
    "                self.bn_modules.append(BatchNorm(in_channels=bn_dim))\n",
    "            if self.skip:\n",
    "                skip_in_dim=inp_dim if i==0 else filters[i-1]\n",
    "                if skip_in_dim!=filters[i]:# y=GatConv(x)+W*x. W*x tranforms x to the same dimension as GatConv(x)\n",
    "                    self.skip_conns.append(nn.Linear(in_features=skip_in_dim,out_features=filters[i],bias=False))\n",
    "                else:# y=GatConv(x)+x\n",
    "                    self.skip_conns.append(nn.Identity())\n",
    "        self.leaky=nn.LeakyReLU()\n",
    "        self.drop=nn.Dropout(p=edge_drop)\n",
    "        \n",
    "    def forward(self,x,adjs):\n",
    "        for i,adj in enumerate(adjs):\n",
    "            if self.skip:\n",
    "                prev_x=x.clone()\n",
    "            x=self.gat_modules[i](x,adj.edge_index.to(device=x.device))\n",
    "            if self.skip:\n",
    "                x+=self.skip_conns[i](prev_x)\n",
    "            x=x[:adj.size[1]]\n",
    "            \n",
    "            if self.bn:\n",
    "                x=self.bn_modules[i](x)\n",
    "            x=self.drop(x)\n",
    "            x=self.leaky(x)\n",
    "        return x\n",
    "    \n",
    "class Fcn(nn.Module):\n",
    "    '''\n",
    "    last layer will not have non-linear activation.\n",
    "    '''\n",
    "    def __init__(self,inp_dim=16,layers=[8,40],bn=True,drop=0.1):\n",
    "        super().__init__()\n",
    "        self.lyrs=[]\n",
    "        for i,j in enumerate(layers):\n",
    "            if i==0:\n",
    "                self.lyrs.append(nn.Linear(inp_dim,layers[i]))\n",
    "            else:\n",
    "                self.lyrs.append(nn.Linear(layers[i-1],layers[i]))\n",
    "            if i!=len(layers)-1:\n",
    "                self.lyrs.append(nn.LeakyReLU())\n",
    "                self.lyrs.append(nn.BatchNorm1d(layers[i]))\n",
    "                self.lyrs.append(nn.Dropout(p=drop))\n",
    "        self.lyrs=nn.Sequential(*self.lyrs)#pass the list to Sequential.   \n",
    "    def forward(self,x):\n",
    "        return self.lyrs(x)\n",
    "\n",
    "class arxiv_classifier(nn.Module):\n",
    "    def __init__(self,gnn_inp_dim=3,gnn_filters=[16,16,16],gnn_heads=[3,3,3],gnn_drop=0.1,gnn_edge_drop=0.1,gnn_bn=True\n",
    "                ,fc_layers=[8,40],fc_bn=True,fc_drop=0.1):\n",
    "        '''\n",
    "        This model applies the Gat followed by the Fcn.\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.gnn=Gat(inp_dim=gnn_inp_dim,filters=gnn_filters,heads=gnn_heads,drop=gnn_drop,\n",
    "                     edge_drop=gnn_edge_drop,bn=gnn_bn)\n",
    "        self.fcn=Fcn(inp_dim=gnn_filters[-1],layers=fc_layers,bn=fc_bn,drop=fc_drop)\n",
    "    def forward(self,x,adjs):\n",
    "        gnn_out=self.gnn(x,adjs)\n",
    "        return self.fcn(gnn_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1024\n",
    "loader=create_dataloder(data,split_idx,batch_size=batch_size,sizes=[-1,-1,-1])\n",
    "device=torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model=arxiv_classifier(gnn_inp_dim=128,gnn_filters=[32,32,32],gnn_heads=[4,4,4])\n",
    "model=model.to(device=device)\n",
    "opt=optim.Adam(model.parameters(),lr=0.01)\n",
    "scheduler=optim.lr_scheduler.CyclicLR(optimizer=opt,base_lr=0.01,max_lr=0.04,step_size_up=len(loader['train'])/2,cycle_momentum=False)\n",
    "closs=nn.CrossEntropyLoss()\n",
    "model_savename='model_gat.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0. Current best accuracy: 0.5513943420920165\n",
      "Epoch 1. Current best accuracy: 0.5748850632571563\n",
      "Epoch 2. Current best accuracy: 0.5813953488372093\n",
      "Epoch 3. Current best accuracy: 0.5903218228799624\n",
      "Epoch 5. Current best accuracy: 0.5928722440350347\n",
      "Epoch 7. Current best accuracy: 0.5940803382663847\n",
      "Epoch 8. Current best accuracy: 0.5968992248062015\n",
      "Epoch 9. Current best accuracy: 0.5970670156716669\n",
      "Epoch 16. Current best accuracy: 0.598946273364878\n",
      "Epoch 17. Current best accuracy: 0.5991140642303433\n",
      "Epoch 18. Current best accuracy: 0.6030403704822309\n",
      "Epoch 28. Current best accuracy: 0.6035101849055338\n",
      "Epoch 36. Current best accuracy: 0.6047853954830699\n",
      "Epoch 39. Current best accuracy: 0.6064297459646297\n",
      "Epoch 43. Current best accuracy: 0.6069666767341186\n",
      "Epoch 53. Current best accuracy: 0.6071009094264909\n",
      "Epoch 60. Current best accuracy: 0.6072351421188631\n",
      "Epoch 75. Current best accuracy: 0.6081412127923755\n",
      "Epoch 80. Current best accuracy: 0.6087452599080506\n",
      "Epoch 86. Current best accuracy: 0.6093493070237256\n",
      "Epoch 151. Current best accuracy: 0.6097520051008423\n"
     ]
    }
   ],
   "source": [
    "lr_lis=[]\n",
    "def accuracy(pred:torch.tensor,truth:torch.tensor):\n",
    "    return sum(pred==truth).item()/len(truth)\n",
    "def one_epoch(data,loader,model,loss_func,opt,sched=None,eval_func=accuracy,train=True):\n",
    "    sum_eval=0\n",
    "    datapoints=0\n",
    "    for size,n_id,adjs in loader:\n",
    "        inp=data.x[n_id].to(device=device)\n",
    "        pred=model(inp,adjs)\n",
    "        truth=data.y[n_id[:size]].to(device=device).squeeze(dim=1)\n",
    "        loss=loss_func(pred,truth)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            sched.step()\n",
    "            opt.zero_grad()\n",
    "            lr_lis.append(opt.param_groups[0]['lr'])\n",
    "        sum_eval+=accuracy(torch.argmax(pred,dim=1),truth)*size#.item converts a 0d tensor to a python number \n",
    "        datapoints+=size\n",
    "    return sum_eval/datapoints\n",
    "        \n",
    "epochs=250\n",
    "best_accuracy=0.0\n",
    "for i in range(epochs):\n",
    "    model.train()\n",
    "    train_loss=one_epoch(data,loader['train'],model,closs,opt,sched=scheduler)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_loss=one_epoch(data,loader['valid'],model,closs,opt,train=False)\n",
    "    if val_loss>best_accuracy:\n",
    "        best_accuracy=val_loss\n",
    "        print(f'Epoch {i}. Current best accuracy: {best_accuracy}')\n",
    "        torch.save(model.state_dict(),model_savename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
